[model]
# Number of up/down blocks at each level of the unet
num_blocks = 2
# Input channnels (number of rows in tensor)
in_channels = 19
# Channels at top level
base_channels = 64
# Increase in channel count at each level
channel_mult = [1, 2, 3, 4, 5]
# Standard deviation of the data
# The model works best if the data is normalized to a std of 0.5 and this argument is left untouched
data_std = 0.5
# Dimension of class labels (in our case, the other random scalar parameters)
label_dim = 9
# The number of grid cells in the simulations
resolution = 128

[training]
# Number of complete passes through the training data to complete
epochs = 10
# Training mini-batch size
batch_size = 512
# Weight decay rate
weight_decay = 0.00
# Frequency in batches with which the model is tested on the validation set
eval_freq = 100
# Exponential moving average factor. Set to 1 to disable
ema = 0.995
# Whether to use automatic mixed precision (float16 where possible)
use_amp = true

# Loss function configuration
[training.loss]
# log(mean) of noise levels to sample
P_mean = -1.2
# log(std) of noise levels to sample
P_std = 1.2
# Whether to include the heterosceladic uncertainty component (`logvar`) in the training loss.
# NOTE: This means the loss can go negative. When plotting loss, we omit this component for ease of visualization.
include_logvar = false

# Optimizer configuration
[training.optimizer]
# Parameters for Adam optimizer
adam_betas = [0.9, 0.995]
# Initial learning rate
lr = 3e-4
# Minimum learning rate, if LR decay enabled
min_lr = 3e-5
# Epochs after which learning rate decay begins. Set to -1 to disable.
lr_decay_start_epochs = -1
# Weight decay rate
weight_decay = 0.00

# Directory configuration
[training.directories]
# Folder to save results in
out_dir = "saved_models/small"
# Training data folder
train_data_dir = "data/normalized"
# test data folder
test_data_dir = "data/val_small"

# Checkpoint configuration. Checkpoints are saved to and loaded from `out_dir`/checkpoint.pth.tar
[training.checkpoints]
# Whether to load from a checkpoint
load_checkpoint = true
# Frequency in batches with which checkpoints are saved. Set to -1 to disable checkpointing
checkpoint_save_freq = 20

